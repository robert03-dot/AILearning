from inltk.inltk import tokenize
from inltk.inltk import setup
"""
Rule-based tokenization:
Rule-based tokenization is a technique where a set of rules is applied
to the input text to split it into tokens.These rules can be based on
different criteria,such as whitespace,punctuation,regex,or language-specific rules.
Here are some common concepts related to rule-based tokenization:

Whitespace tokenization
This approach splits the input text based on whitespace characters such as space, tab, or newline.

For example, the sentence : "This is a sample text."
would be split into the following tokens: "This", "is", "a", "sample", and "text."

Steps for Rule-based tokenization:
Load the input text: The input text can be loaded from a file or entered by the user.
Define the tokenization rules: Based on the type of tokenization required, define the rules to split the input text into tokens. These rules can be based on whitespace, punctuation, regular expressions, or language-specific rules.
Apply the rules to the input text: Use the defined rules to split the input text into tokens.
Output the tokens: Output the tokens generated by the tokenization process.
"""
import re
from tokenize import tokenize
from turtledemo.clock import setup

text = "The quick brown fox jumps over the lazy dog."
tokens = text.split()
print(tokens)

"""
Regular expression tokenization
This approach used regex to split the text based on a pattern.This is 
mainly used when we have to find some specific type of patterns in text
like email id,phone number,order id,currency,etc.

For example, the regular expression "[\w]+-[\w]+-[\w]+" will match the "Geeks-for-Geeks" 
and ([\w\.-]+@[\w]+.[\w]+) will match the email id. from
"Hello, I am working at Geeks-for-Geeks and my email is pawan.gunjan123@geeksforgeeks.com." 
"""

# Load the input text
text = "Hello, I am working at Geeks-for-Geeks and my email is pawangunjan23@geeksforgeeks.com."

# Define the regular expression pattern
p = '([\w]+-[\w]+-[\w]+)|([\w\.-]+@[\w]+.[\w]+)'
matches = re.findall(p, text)
for match in matches:
    if match[0]:
        print(f"Company name: {match[0]}")
    else:
        print(f"Email address: {match[1]}")

##
# Punctuation tokenization
# This approach splits the input text based on punctuation characters such
# as period,comma or semicolon.
#
# For example, the sentence "Hello Geeks! How can I help you?"
# would be split into the following tokens: 'Hello', 'Geeks', 'How', 'can', 'I', 'help', 'you'#

text = "Hello Geeks! How can I help you?"
patter = r'\W+'
result = re.sub(patter, ' ', text)
tokens = re.findall(r'\b\w+\b|[^\w\s]', result)
print(tokens)

"""
Language-specific tokenization
This approach uses langauge-specific rules to split the input text into
tokens.For example,in some languages,words can be concatenated wihtout
spaces,such as in German.Therefore,language-specific rules are needed
to split the input text into meaningful tokens.
"""

setup('sa')
Text = "'ॐ भूर्भव: स्व: तत्सवितुर्वरेण्यं भर्गो देवस्य धीमहि धियो यो न: प्रचोदयात्।'"
tokenize(Text,"sa")